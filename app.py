import streamlit as st
import pandas as pd
import numpy as np
import requests
import os
import holidays
import itertools
from datetime import datetime, timedelta
from fpdf import FPDF
import subprocess
import geopandas as gpd
import json
import requests
import io
import json
from shapely.geometry import Point
from scipy.spatial import cKDTree

st.set_page_config(page_title="Veri GÃ¼ncelleme", layout="wide")
st.title("ðŸ“¦ GÃ¼nlÃ¼k SuÃ§ Verisi Ä°ÅŸleme ve Ã–zetleme Paneli")

DOWNLOAD_URL = "https://github.com/cem5113/crime_prediction_data/releases/download/latest/sf_crime.csv"
DOWNLOAD_911_URL = "https://github.com/cem5113/crime_prediction_data/releases/download/v1.0.1/sf_911_last_5_year.csv"
DOWNLOAD_311_URL = "https://github.com/cem5113/crime_prediction_data/releases/download/v1.0.2/sf_311_last_5_years.csv"
POPULATION_PATH = "sf_population.csv"
DOWNLOAD_BUS_URL = "https://github.com/cem5113/crime_prediction_data/raw/main/sf_bus_stops_with_geoid.csv"
DOWNLOAD_TRAIN_URL = "https://github.com/cem5113/crime_prediction_data/raw/main/sf_train_stops_with_geoid.csv"
DOWNLOAD_POIS_URL = "https://github.com/cem5113/crime_prediction_data/raw/main/sf_pois.geojson"
RISKY_POIS_JSON_PATH = "risky_pois_dynamic.json"
DOWNLOAD_POLICE_URL = "https://github.com/cem5113/crime_prediction_data/raw/main/sf_police_stations.csv"
DOWNLOAD_GOV_URL = "https://github.com/cem5113/crime_prediction_data/raw/main/sf_government_buildings.csv"
DOWNLOAD_WEATHER_URL = "https://github.com/cem5113/crime_prediction_data/releases/download/latest/sf_weather_5years.csv"

def update_bus_data_if_needed():
    import geopandas as gpd
    from shapely.geometry import Point
    import os

    api_url = "https://data.sfgov.org/resource/i28k-bkz6.json"
    timestamp_file = "bus_stops_last_update.txt"

    def is_month_passed(file):
        if os.path.exists(file):
            with open(file, "r") as f:
                last = f.read().strip()
            try:
                last_date = datetime.strptime(last, "%Y-%m-%d")
                return (datetime.today() - last_date).days >= 30
            except:
                return True
        return True

    if is_month_passed(timestamp_file):
        try:
            response = requests.get(api_url)
            if response.status_code == 200:
                bus_data = response.json()
                df = pd.DataFrame(bus_data)

                # latitude / longitude sÃ¼tunlarÄ±nÄ± float olarak al
                df = df.dropna(subset=["latitude", "longitude"])
                df["stop_lat"] = df["latitude"].astype(float)
                df["stop_lon"] = df["longitude"].astype(float)

                # GeoDataFrame'e Ã§evir
                gdf_stops = gpd.GeoDataFrame(
                    df,
                    geometry=gpd.points_from_xy(df["stop_lon"], df["stop_lat"]),
                    crs="EPSG:4326"
                )

                # Census bloklarÄ±nÄ± oku
                census_path = "sf_census_blocks_with_population.geojson"
                gdf_blocks = gpd.read_file(census_path)[["GEOID", "geometry"]].to_crs("EPSG:4326")

                # Spatial join
                gdf_joined = gpd.sjoin(gdf_stops, gdf_blocks, how="left", predicate="within")
                gdf_joined["GEOID"] = gdf_joined["GEOID"].astype(str).str.zfill(11)
                gdf_joined.drop(columns=["geometry", "index_right"], errors="ignore").to_csv("sf_bus_stops_with_geoid.csv", index=False)

                # Tarih kaydet
                with open(timestamp_file, "w") as f:
                    f.write(datetime.today().strftime("%Y-%m-%d"))

                st.success("ðŸšŒ OtobÃ¼s duraklarÄ± Socrata API'den indirildi ve GEOID ile eÅŸleÅŸtirildi.")
            else:
                st.warning(f"âš ï¸ OtobÃ¼s verisi indirilemedi: {response.status_code}")
        except Exception as e:
            st.error(f"âŒ OtobÃ¼s verisi gÃ¼ncellenemedi: {e}")
    else:
        st.info("ðŸ“… OtobÃ¼s verisi bu ay zaten gÃ¼ncellendi.")

def update_train_data_if_needed():
    import zipfile
    import geopandas as gpd
    import pandas as pd
    import os
    from datetime import datetime

    zip_path = "bart_gtfs.zip"
    timestamp_file = "train_stops_last_update.txt"

    def is_month_passed(file):
        if os.path.exists(file):
            with open(file, "r") as f:
                last = f.read().strip()
            try:
                last_date = datetime.strptime(last, "%Y-%m-%d")
                return (datetime.today() - last_date).days >= 30
            except:
                return True
        return True

    if is_month_passed(timestamp_file):
        try:
            response = requests.get(DOWNLOAD_TRAIN_URL)
            if response.status_code == 200:
                with open(zip_path, "wb") as f:
                    f.write(response.content)
                with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                    zip_ref.extract("stops.txt", ".")
                os.rename("stops.txt", "sf_train_stops.csv")
                with open(timestamp_file, "w") as f:
                    f.write(datetime.today().strftime("%Y-%m-%d"))
                st.success("ðŸš† BART tren duraklarÄ± gÃ¼ncellendi (sf_train_stops.csv)")

                # === GEOID EÅžLEME ===
                train_df = pd.read_csv("sf_train_stops.csv")
                gdf_stops = gpd.GeoDataFrame(
                    train_df,
                    geometry=gpd.points_from_xy(train_df["stop_lon"], train_df["stop_lat"]),
                    crs="EPSG:4326"
                )

                census_path = "https://github.com/cem5113/crime_prediction_data/raw/main/sf_pois.geojson"
                gdf_blocks = gpd.read_file(census_path)[["GEOID", "geometry"]].to_crs("EPSG:4326")

                gdf_joined = gpd.sjoin(gdf_stops, gdf_blocks, how="left", predicate="within")
                gdf_joined["GEOID"] = gdf_joined["GEOID"].astype(str).str.zfill(11)
                gdf_joined.drop(columns=["geometry", "index_right"], errors="ignore").to_csv("sf_train_stops_with_geoid.csv", index=False)

                st.success("ðŸ“Œ GEOID ile eÅŸleÅŸtirilmiÅŸ tren duraklarÄ± oluÅŸturuldu (sf_train_stops_with_geoid.csv)")

            else:
                st.warning(f"âš ï¸ Tren verisi indirilemedi: {response.status_code}")
        except Exception as e:
            st.error(f"âŒ Tren verisi indirme hatasÄ±: {e}")
    else:
        st.info("ðŸ“… Tren verisi bu ay zaten gÃ¼ncellenmiÅŸ.")

def update_pois_if_needed():
    import os
    import pandas as pd
    import streamlit as st
    from datetime import datetime
    import update_pois  # ðŸ” SUBPROCESS YERÄ°NE DÄ°REKT MODÃœL GÄ°BÄ° KULLAN

    timestamp_file = "poi_last_update.txt"

    def is_month_passed(file):
        if os.path.exists(file):
            with open(file, "r") as f:
                last = f.read().strip()
            try:
                last_date = datetime.strptime(last, "%Y-%m-%d")
                return (datetime.today() - last_date).days >= 30
            except:
                return True
        return True

    if is_month_passed(timestamp_file):
        try:
            st.info("ðŸ“¥ POI verisi gÃ¼ncelleniyor...")
            update_pois.process_pois()
            update_pois.calculate_dynamic_risk()

            with open(timestamp_file, "w") as f:
                f.write(datetime.today().strftime("%Y-%m-%d"))

            st.success("âœ… POI verisi baÅŸarÄ±yla gÃ¼ncellendi.")
        except Exception as e:
            st.error(f"âŒ POI gÃ¼ncelleme hatasÄ±: {e}")
    else:
        st.info("ðŸ“… POI verisi bu ay zaten gÃ¼ncellendi.")

def update_police_and_gov_buildings_if_needed():
    import requests
    import geopandas as gpd
    import pandas as pd
    import os
    from datetime import datetime
    from shapely.geometry import Point

    timestamp_file = "police_gov_last_update.txt"
    overpass_url = "http://overpass-api.de/api/interpreter"

    def is_month_passed(file):
        if os.path.exists(file):
            with open(file, "r") as f:
                last = f.read().strip()
            try:
                last_date = datetime.strptime(last, "%Y-%m-%d")
                return (datetime.today() - last_date).days >= 30
            except:
                return True
        return True

    if is_month_passed(timestamp_file):
        try:
            st.write("ðŸŒ Overpass API'den veri Ã§ekiliyor...")

            # === Overpass SorgularÄ± ===
            queries = {
                "police": """
                [out:json][timeout:60];
                (
                  node["amenity"="police"](37.70,-123.00,37.83,-122.35);
                  way["amenity"="police"](37.70,-123.00,37.83,-122.35);
                );
                out center;
                """,
                "government": """
                [out:json][timeout:60];
                (
                  node["amenity"="townhall"](37.70,-123.00,37.83,-122.35);
                  node["office"="government"](37.70,-123.00,37.83,-122.35);
                );
                out center;
                """
            }

            def fetch_pois(name, query):
                response = requests.post(overpass_url, data={"data": query})
                data = response.json()["elements"]

                rows = []
                for el in data:
                    lat = el.get("lat") or el.get("center", {}).get("lat")
                    lon = el.get("lon") or el.get("center", {}).get("lon")
                    if lat and lon:
                        tags = el.get("tags", {})
                        rows.append({
                            "id": el["id"],
                            "lat": lat,
                            "lon": lon,
                            "name": tags.get("name", ""),
                            "type": tags.get("amenity") or tags.get("office", ""),
                        })

                df = pd.DataFrame(rows)
                df["latitude"] = df["lat"]
                df["longitude"] = df["lon"]
                df = df.drop(columns=["lat", "lon"])
                gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df["longitude"], df["latitude"]), crs="EPSG:4326")
                return gdf

            gdf_police = fetch_pois("police", queries["police"])
            gdf_police.to_csv("sf_police_stations.csv", index=False)
            st.success("âœ… sf_police_stations.csv indirildi ve kaydedildi.")

            gdf_gov = fetch_pois("government", queries["government"])
            gdf_gov.to_csv("sf_government_buildings.csv", index=False)
            st.success("âœ… sf_government_buildings.csv indirildi ve kaydedildi.")

            with open(timestamp_file, "w") as f:
                f.write(datetime.today().strftime("%Y-%m-%d"))

        except Exception as e:
            st.error(f"âŒ Polis/kamu binasÄ± gÃ¼ncelleme hatasÄ±: {e}")
    else:
        st.info("ðŸ“… Polis ve kamu binasÄ± verisi bu ay zaten gÃ¼ncellendi.")

def update_weather_data():
    st.info("ðŸŒ¦ï¸ Hava durumu verisi kontrol ediliyor...")
    try:
        save_path = "sf_weather_5years.csv"
        station_id = "USW00023234"
        end_date = datetime.today().date()
        start_date = end_date - pd.Timedelta(days=5*365)

        url = (
            "https://www.ncei.noaa.gov/access/services/data/v1"
            f"?dataset=daily-summaries"
            f"&stations={station_id}"
            f"&startDate={start_date}"
            f"&endDate={end_date}"
            f"&dataTypes=TMAX,TMIN,PRCP"
            f"&format=csv"
        )

        response = requests.get(url)
        if response.status_code == 200:
            df_new = pd.read_csv(io.StringIO(response.text))
            df_new["DATE"] = pd.to_datetime(df_new["DATE"])

            if os.path.exists(save_path):
                df_old = pd.read_csv(save_path)
                df_old["DATE"] = pd.to_datetime(df_old["DATE"])
                df_combined = pd.concat([df_old, df_new])
                df_combined = df_combined.drop_duplicates(subset=["DATE"])
            else:
                df_combined = df_new

            df_filtered = df_combined[df_combined["DATE"] >= pd.to_datetime(start_date)]
            df_filtered.to_csv(save_path, index=False)

            st.success(f"âœ… Hava durumu gÃ¼ncellendi: {start_date} â†’ {end_date}")
        else:
            st.warning(f"âŒ NOAA'dan veri Ã§ekilemedi: {response.status_code}")
    except Exception as e:
        st.error(f"âŒ Hava durumu gÃ¼ncellenemedi: {e}")

def create_pdf_report(file_name, row_count_before, nan_cols, row_count_after, removed_rows):
    now = datetime.now()
    timestamp = now.strftime("%d.%m.%Y %H:%M:%S")

    if not nan_cols.empty:
        nan_parts = [f"- {col}: {count}" for col, count in nan_cols.items()]
        nan_text = " ".join(nan_parts)
    else:
        nan_text = "Yok"

    summary = (
        f"- Tarih/Saat: {timestamp}; "
        f"Dosya: {file_name} ; "
        f"Toplam satir sayisi: {row_count_before:,}; "
        f"NaN iceren sutunlar: {nan_text}; "
        f"Revize satir sayisi: {row_count_after:,}; "
        f"Silinen eski tarihli satir sayisi: {removed_rows}"
    )

    pdf = FPDF()
    pdf.add_page()
    pdf.set_font("Arial", size=12)
    pdf.multi_cell(0, 10, txt=summary.encode("latin1", "replace").decode("latin1"))

    output_name = f"report_{now.strftime('%Y%m%d_%H%M%S')}.pdf"
    pdf.output(output_name)
    return output_name

if st.button("ðŸ“¥ sf_crime.csv indir, zenginleÅŸtir ve Ã¶zetle"):
    with st.spinner("â³ Ä°ÅŸlem devam ediyor... LÃ¼tfen bekleyin. Bu birkaÃ§ dakika sÃ¼rebilir."):
        try:
            response = requests.get(DOWNLOAD_URL)
            if response.status_code == 200:
                with open("sf_crime.csv", "wb") as f:
                    f.write(response.content)
                st.success("âœ… sf_crime.csv baÅŸarÄ±yla indirildi.")

                update_train_data_if_needed()
                update_bus_data_if_needed() 
                update_pois_if_needed()
                update_pois_if_needed()
                update_weather_data()
                update_police_and_gov_buildings_if_needed()
                if os.path.exists("sf_pois_cleaned_with_geoid.csv"):
                    st.success("âœ… POI CSV dosyasÄ± baÅŸarÄ±yla oluÅŸturuldu.")
                else:
                    st.error("âŒ POI CSV dosyasÄ± eksik!")
            
                if os.path.exists("risky_pois_dynamic.json"):
                    st.success("âœ… POI risk skoru dosyasÄ± oluÅŸturuldu.")
                else:
                    st.error("âŒ Risk skoru JSON dosyasÄ± bulunamadÄ±.")

                # 911 verisini indir
                df_911 = None  # Ã¶n tanÄ±m
                try:
                    response_911 = requests.get(DOWNLOAD_911_URL)
                    if response_911.status_code == 200:
                        with open("sf_911_last_5_year.csv", "wb") as f:
                            f.write(response_911.content)
                        st.success("âœ… sf_911_last_5_year.csv baÅŸarÄ±yla indirildi.")
                        
                        # ðŸ‘â€ðŸ—¨ Ä°Ã§eriÄŸi gÃ¶ster
                        df_911 = pd.read_csv("sf_911_last_5_year.csv")
                        st.write("ðŸ“Ÿ 911 Verisi Ä°lk 5 SatÄ±r")
                        st.dataframe(df_911.head())
                        st.write("ðŸ“Œ 911 SÃ¼tunlarÄ±:")
                        st.write(df_911.columns.tolist())
                    else:
                        st.warning(f"âš ï¸ sf_911_last_5_year.csv indirilemedi: {response_911.status_code}")
                except Exception as e:
                    st.error(f"âŒ 911 verisi indirilemedi: {e}")

                # 311 verisini oku 
                df_311 = None
                try:
                    response_311 = requests.get(DOWNLOAD_311_URL)
                    if response_311.status_code == 200:
                        with open("sf_311_last_5_years.csv", "wb") as f:
                            f.write(response_311.content)
                        st.success("âœ… sf_311_last_5_years.csv baÅŸarÄ±yla indirildi.")
                
                        df_311 = pd.read_csv("sf_311_last_5_years.csv")
                        df_311["date"] = pd.to_datetime(df_311["date"]).dt.date
                
                        st.write("ðŸ“Ÿ 311 Verisi Ä°lk 5 SatÄ±r")
                        st.dataframe(df_311.head())
                        st.write("ðŸ“Œ 311 SÃ¼tunlarÄ±:")
                        st.write(df_311.columns.tolist())
                
                    else:
                        st.warning(f"âš ï¸ sf_311_last_5_years.csv indirilemedi: {response_311.status_code}")
                except Exception as e:
                    st.error(f"âŒ 311 verisi yÃ¼klenemedi: {e}")
                    
                # SuÃ§ verisini oku
                df = pd.read_csv("sf_crime.csv", low_memory=False)
                original_row_count = len(df)

                # SuÃ§ verisini oku
                df = pd.read_csv("sf_crime.csv", low_memory=False)
                original_row_count = len(df)

                # ðŸ” POI Risk ve YoÄŸunluk Ã–zelliklerini Ekle
                try:
                    df_poi = pd.read_csv("sf_pois_cleaned_with_geoid.csv")
                    
                    with open("risky_pois_dynamic.json") as f:
                        risk_dict = json.load(f)
                    
                    df_poi["risk_score"] = df_poi["poi_subcategory"].map(risk_dict).fillna(0)

                    poi_features = df_poi.groupby("GEOID").agg(
                        poi_total_count=("id", "count"),
                        risky_poi_score=("risk_score", "mean")
                    ).reset_index()

                    df["GEOID"] = df["GEOID"].astype(str).str.zfill(11)
                    poi_features["GEOID"] = poi_features["GEOID"].astype(str).str.zfill(11)
                    df = df.merge(poi_features, on="GEOID", how="left")

                    st.success("âœ… POI yoÄŸunluÄŸu ve risk skoru baÅŸarÄ±yla eklendi.")
                    st.write("ðŸ“ Ã–rnek POI verisi:")
                    st.dataframe(df[["GEOID", "poi_total_count", "risky_poi_score"]].drop_duplicates().head())

                except Exception as e:
                    st.warning(f"âš ï¸ POI verisi eklenemedi: {e}")
                
                try:
                    df_poi = pd.read_csv("sf_pois_cleaned_with_geoid.csv")
                    df_poi["risk_score"] = df_poi["poi_subcategory"].map(risk_dict).fillna(0)
                    
                    gdf_crime = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df["longitude"], df["latitude"]), crs="EPSG:4326").to_crs(3857)
                    gdf_poi = gpd.GeoDataFrame(df_poi, geometry=gpd.points_from_xy(df_poi["lon"], df_poi["lat"]), crs="EPSG:4326").to_crs(3857)
                
                    # Genel POI mesafesi
                    poi_coords = np.vstack([gdf_poi.geometry.x, gdf_poi.geometry.y]).T
                    crime_coords = np.vstack([gdf_crime.geometry.x, gdf_crime.geometry.y]).T
                    poi_tree = cKDTree(poi_coords)
                    df["distance_to_poi"], _ = poi_tree.query(crime_coords, k=1)
                
                    # Riskli POIâ€™lere mesafe
                    risky_poi = gdf_poi[gdf_poi["risk_score"] > 0]
                    if not risky_poi.empty:
                        risky_coords = np.vstack([risky_poi.geometry.x, risky_poi.geometry.y]).T
                        risky_tree = cKDTree(risky_coords)
                        df["distance_to_high_risk_poi"], _ = risky_tree.query(crime_coords, k=1)
                    else:
                        df["distance_to_high_risk_poi"] = np.nan
                
                    # POI Risk yoÄŸunluÄŸu (GEOID bazlÄ±)
                    risk_density = df_poi.groupby("GEOID")["risk_score"].mean().reset_index(name="poi_risk_density")
                    df["GEOID"] = df["GEOID"].astype(str).str.zfill(11)
                    risk_density["GEOID"] = risk_density["GEOID"].astype(str).str.zfill(11)
                
                    st.success("âœ… POI mesafe ve risk yoÄŸunluÄŸu eklendi.")
                except Exception as e:
                    st.error(f"âŒ POI mesafe/risk hesaplama hatasÄ±: {e}")

                # NÃ¼fus verisini oku
                if os.path.exists(POPULATION_PATH):
                    df_pop = pd.read_csv(POPULATION_PATH)
                    df["GEOID"] = df["GEOID"].astype(str).str.extract(r'(\d+)')[0].str.zfill(11)
                    df_pop["GEOID"] = df_pop["GEOID"].astype(str).str.zfill(11)
                    df = pd.merge(df, df_pop, on="GEOID", how="left")
                    df["population"] = df["population"].fillna(0).astype(int)
                    st.success("âœ… NÃ¼fus verisi eklendi.")
                    st.write("ðŸ‘¥ NÃ¼fus Ã¶rnek verisi:")
                    st.dataframe(df[["GEOID", "population"]].drop_duplicates().head())
                else:
                    st.warning("âš ï¸ NÃ¼fus verisi (sf_population.csv) bulunamadÄ±.")

                # OtobÃ¼s durak verisini indir
                df_bus = None
                try:
                    response_bus = requests.get(DOWNLOAD_BUS_URL)
                    if response_bus.status_code == 200:
                        with open("sf_bus_stops.csv", "wb") as f:
                            f.write(response_bus.content)
                        st.success("âœ… sf_bus_stops.csv baÅŸarÄ±yla indirildi.")
                        df_bus = pd.read_csv("sf_bus_stops.csv").dropna(subset=["stop_lat", "stop_lon"])
                        st.write("ðŸšŒ OtobÃ¼s Verisi Ä°lk 5 SatÄ±r:")
                        st.dataframe(df_bus.head())
                    else:
                        st.warning(f"âš ï¸ sf_bus_stops.csv indirilemedi: {response_bus.status_code}")
                except Exception as e:
                    st.error(f"âŒ OtobÃ¼s verisi indirilemedi: {e}")

                # NaN Ã¶zetle
                nan_summary = df.isna().sum()
                nan_cols = nan_summary[nan_summary > 0]
                removed_rows = 0  # HenÃ¼z satÄ±r silinmedi

                # PDF rapor oluÅŸtur
                removed_rows = original_row_count - len(df)
                report_path = create_pdf_report("sf_crime.csv", original_row_count, nan_cols, len(df), removed_rows)
                with open(report_path, "rb") as f:
                    st.download_button("ðŸ“„ PDF Raporu Ä°ndir", f, file_name=report_path, mime="application/pdf")

            else:
                st.error(f"âŒ sf_crime.csv indirilemedi, HTTP kodu: {response.status_code}")
                st.stop()  # HatalÄ± indirme varsa durdur
        except Exception as e:
            st.error(f"âŒ Hata oluÅŸtu: {e}")

            # Enrichment
            df["datetime"] = pd.to_datetime(df["date"].astype(str) + " " + df["time"].astype(str), errors="coerce")
            df = df.dropna(subset=["datetime"])
            df["datetime"] = df["datetime"].dt.floor("h")
            df["event_hour"] = df["datetime"].dt.hour
            df["date"] = df["datetime"].dt.date
            df["month"] = df["datetime"].dt.month
            df["year"] = df["datetime"].dt.year
            df["day_of_week"] = df["datetime"].dt.dayofweek
            df["is_night"] = df["event_hour"].apply(lambda x: 1 if (x >= 20 or x < 4) else 0)
            df["is_weekend"] = df["day_of_week"].apply(lambda x: 1 if x >= 5 else 0)
            years = df["year"].dropna().astype(int).unique()
            us_holidays = pd.to_datetime(list(holidays.US(years=years).keys()))
            df["is_holiday"] = df["date"].isin(us_holidays).astype(int)
            df["latlon"] = df["latitude"].round(5).astype(str) + "_" + df["longitude"].round(5).astype(str)
            df["is_repeat_location"] = df.duplicated("latlon").astype(int)
            df.drop(columns=["latlon"], inplace=True)
            df["is_school_hour"] = df["event_hour"].apply(lambda x: 1 if 7 <= x <= 16 else 0)
            df["is_business_hour"] = df.apply(lambda x: 1 if (9 <= x["event_hour"] < 18 and x["day_of_week"] < 5) else 0, axis=1)
            season_map = {12: "Winter", 1: "Winter", 2: "Winter", 3: "Spring", 4: "Spring", 5: "Spring", 6: "Summer", 7: "Summer", 8: "Summer", 9: "Fall", 10: "Fall", 11: "Fall"}
            df["season"] = df["month"].map(season_map)

            # 911 verilerini yÃ¼kle ve birleÅŸtir
            if os.path.exists("sf_911_last_5_year.csv"):
                df_911 = pd.read_csv("sf_911_last_5_year.csv")
                df_911["date"] = pd.to_datetime(df_911["date"]).dt.date
                df["hour_range"] = (df["event_hour"] // 3) * 3
                df["hour_range"] = df["hour_range"].astype(str) + "-" + (df["event_hour"] // 3 * 3 + 3).astype(str)
                
                # BirleÅŸtir
                df = pd.merge(df, df_911, on=["GEOID", "date", "hour_range"], how="left")
                
                # Yeni sÃ¼tunlarÄ± gÃ¶zlemle
                cols_911 = [col for col in df.columns if "911" in col or "request" in col]
                st.write("ðŸ” 911 SÃ¼tunlarÄ±:")
                st.write(cols_911)
                st.write("ðŸ§¯ 911 NaN SayÄ±larÄ±:")
                st.write(df[cols_911].isna().sum())
            
                # Eksik olanlarÄ± 0 yap
                for col in cols_911:
                    df[col] = df[col].fillna(0)
                
                # 311 verisini birleÅŸtir
                if df_311 is not None:
                    if "hour_range" not in df_311.columns and "time" in df_311.columns:
                        df_311["datetime"] = pd.to_datetime(df_311["date"].astype(str) + " " + df_311["time"].astype(str), errors="coerce")
                        df_311["hour"] = df_311["datetime"].dt.hour
                        df_311["hour_range"] = (df_311["hour"] // 3) * 3
                        df_311["hour_range"] = df_311["hour_range"].astype(str) + "-" + (df_311["hour_range"] + 3).astype(str)
                
                    # Merge Ã¶ncesi tip dÃ¼zeltmeleri
                    df["GEOID"] = df["GEOID"].astype(str).str.extract(r"(\d+)")[0].str.zfill(11)
                    df["GEOID"] = df["GEOID"].apply(lambda x: str(int(x)).zfill(11) if pd.notna(x) else None)
                    df_311["GEOID"] = df_311["GEOID"].apply(lambda x: str(int(float(x))).zfill(11) if pd.notna(x) else None)
                    df["date"] = pd.to_datetime(df["date"]).dt.date
                    df_311["date"] = pd.to_datetime(df_311["date"]).dt.date
                    df["hour_range"] = df["hour_range"].astype(str)
                    df_311["hour_range"] = df_311["hour_range"].astype(str)
                
                    # Aggregate: saat aralÄ±ÄŸÄ± baÅŸÄ±na toplam Ã§aÄŸrÄ±
                    agg_311 = df_311.groupby(["GEOID", "date", "hour_range"]).size().reset_index(name="311_request_count")
                    df = pd.merge(df, agg_311, on=["GEOID", "date", "hour_range"], how="left")
                    df["311_request_count"] = df["311_request_count"].fillna(0)
                
                    # Ek sÃ¼tunlarÄ± (Ã¶rneÄŸin category vs.) merge et (Ã¶rnek kayÄ±t Ã¼zerinden)
                    meta_cols = ["GEOID", "date", "hour_range", "category", "subcategory"]
                    df_311_meta = df_311[meta_cols].drop_duplicates()
                    df = pd.merge(df, df_311_meta, on=["GEOID", "date", "hour_range"], how="left")
                
                    # GÃ¶stermek iÃ§in:
                    cols_311 = [col for col in df.columns if "311" in col or col in ["category", "subcategory"]]
                    st.write("ðŸ” 311 SÃ¼tunlarÄ±:")
                    st.write(cols_311)
                    st.write("ðŸ§¯ 311 NaN SayÄ±larÄ±:")
                    st.write(df[cols_311].isna().sum())
                
                    for col in cols_311:
                        df[col] = df[col].fillna(0) if df[col].dtype != 'object' else df[col].fillna("Unknown")
                        
            df = df.sort_values(by=["GEOID", "datetime"]).reset_index(drop=True)

            # En yakÄ±n otobÃ¼s duraÄŸÄ±na mesafe ve durak sayÄ±sÄ±
            if df_bus is not None:
                try:
                    import geopandas as gpd
                    from shapely.geometry import Point
                    from scipy.spatial import cKDTree
                    import numpy as np
            
                    gdf_crime = gpd.GeoDataFrame(
                        df,
                        geometry=gpd.points_from_xy(df["longitude"], df["latitude"]),
                        crs="EPSG:4326"
                    ).to_crs(epsg=3857)
            
                    gdf_bus = gpd.GeoDataFrame(
                        df_bus,
                        geometry=gpd.points_from_xy(df_bus["stop_lon"], df_bus["stop_lat"]),
                        crs="EPSG:4326"
                    ).to_crs(epsg=3857)
            
                    crime_coords = np.vstack([gdf_crime.geometry.x, gdf_crime.geometry.y]).T
                    bus_coords = np.vstack([gdf_bus.geometry.x, gdf_bus.geometry.y]).T
                    tree = cKDTree(bus_coords)
                    distances, _ = tree.query(crime_coords, k=1)
                    df["distance_to_bus"] = distances
            
                    dynamic_radius = np.percentile(distances, 75)
                    def count_stops(pt):
                        return gdf_bus.distance(pt).lt(dynamic_radius).sum()
            
                    df["bus_stop_count"] = gdf_crime.geometry.apply(count_stops)
            
                    st.success("âœ… OtobÃ¼s mesafesi ve durak sayÄ±sÄ± eklendi.")
                    st.write(df[["GEOID", "distance_to_bus", "bus_stop_count"]].head())
                except Exception as e:
                    st.error(f"âŒ OtobÃ¼s entegrasyon hatasÄ±: {e}")

                # === ðŸš† En YakÄ±n Tren DuraÄŸÄ± ve DuraÄŸa UzaklÄ±k ===
                try:
                    import geopandas as gpd
                    from shapely.geometry import Point
                    from scipy.spatial import cKDTree
                    import numpy as np
                
                    # 1. SuÃ§ verisini GeoDataFrame'e Ã§evir
                    gdf_crime = gpd.GeoDataFrame(
                        df,
                        geometry=gpd.points_from_xy(df["longitude"], df["latitude"]),
                        crs="EPSG:4326"
                    ).to_crs(epsg=3857)
                
                    # 2. Tren duraklarÄ±nÄ± oku ve GeoDataFrame'e Ã§evir
                    if os.path.exists("sf_train_stops_with_geoid.csv"):
                        df_train = pd.read_csv("sf_train_stops_with_geoid.csv").dropna(subset=["stop_lat", "stop_lon"])
                        gdf_train = gpd.GeoDataFrame(
                            df_train,
                            geometry=gpd.points_from_xy(df_train["stop_lon"], df_train["stop_lat"]),
                            crs="EPSG:4326"
                        ).to_crs(epsg=3857)
                
                        # 3. KDTree ile mesafe hesapla
                        crime_coords = np.vstack([gdf_crime.geometry.x, gdf_crime.geometry.y]).T
                        train_coords = np.vstack([gdf_train.geometry.x, gdf_train.geometry.y]).T
                        tree = cKDTree(train_coords)
                        distances, _ = tree.query(crime_coords, k=1)
                        df["distance_to_train"] = distances
                
                        # 4. Belirli bir yarÄ±Ã§apta (Ã¶rn. 500m) tren duraÄŸÄ± sayÄ±sÄ±nÄ± hesapla
                        radius = 500  # metre
                        df["train_stop_count"] = gdf_crime.geometry.apply(lambda pt: gdf_train.distance(pt).lt(radius).sum())
                
                        st.success("ðŸš† Tren mesafesi ve durak sayÄ±sÄ± eklendi.")
                        st.write(df[["GEOID", "distance_to_train", "train_stop_count"]].head())
                    else:
                        st.warning("âš ï¸ sf_train_stops_with_geoid.csv bulunamadÄ±. Tren duraklarÄ± eklenmedi.")
                
                except Exception as e:
                    st.error(f"âŒ Tren entegrasyon hatasÄ±: {e}")

            for col in ["past_7d_crimes", "crime_count_past_24h", "crime_count_past_48h", "crime_trend_score", "prev_crime_1h", "prev_crime_2h", "prev_crime_3h"]:
                df[col] = 0

            for geoid, group in df.groupby("GEOID"):
                times = pd.to_datetime(group["datetime"]).values.astype("datetime64[ns]")
                event_hours = group["event_hour"].values
                idx = group.index
                deltas = times[:, None] - times[None, :]

                df.loc[idx, "past_7d_crimes"] = ((deltas > np.timedelta64(0, 'ns')) & (deltas <= np.timedelta64(7, 'D'))).sum(axis=1)
                df.loc[idx, "crime_count_past_24h"] = ((deltas > np.timedelta64(0, 'ns')) & (deltas <= np.timedelta64(1, 'D'))).sum(axis=1)
                df.loc[idx, "crime_count_past_48h"] = ((deltas > np.timedelta64(0, 'ns')) & (deltas <= np.timedelta64(2, 'D'))).sum(axis=1)
                df.loc[idx, "crime_trend_score"] = [((times[:i] >= t - np.timedelta64(7, 'D')) & (event_hours[:i] == h)).sum() for i, (t, h) in enumerate(zip(times, event_hours))]

                for lag in [1, 2, 3]:
                    lag_col = f"prev_crime_{lag}h"
                    df.loc[idx, lag_col] = [1 if ((times[:i] >= t - np.timedelta64(lag, 'h')) & (times[:i] < t)).sum() > 0 else 0 for i, t in enumerate(times)]

            # === Ã–zetleme ===
            df["event_hour"] = df["event_hour"].astype(int)
            df["day_of_week"] = df["datetime"].dt.dayofweek
            df["month"] = df["datetime"].dt.month
            df["season"] = df["month"].map(season_map)

            group_cols = ["GEOID", "season", "day_of_week", "event_hour"]
            mean_cols = ["latitude", "longitude", "past_7d_crimes", "crime_count_past_24h", "crime_count_past_48h", "crime_trend_score", "prev_crime_1h", "prev_crime_2h", "prev_crime_3h"]
            mode_cols = [
                "is_weekend", "is_night", "is_holiday", "is_repeat_location",
                "is_school_hour", "is_business_hour", "year", "month",
                "distance_to_police_range", "distance_to_government_building_range",
                "is_near_police", "is_near_government"
            ]
            
            mean_cols.extend([
                col for col in df.columns if "911" in col or "request" in col
            ])
            mean_cols.extend(["distance_to_bus", "bus_stop_count"])
            mean_cols.extend([
                col for col in df.columns if "311" in col
            ])
            mean_cols.extend([
                "poi_total_count", "risky_poi_score", "distance_to_high_risk_poi",
                "distance_to_poi", "poi_risk_density",
                "distance_to_police", "distance_to_government_building"
            ])
            mean_cols.extend([
                "temp_max",               # Maksimum sÄ±caklÄ±k
                "temp_min",               # Minimum sÄ±caklÄ±k
                "precipitation_mm",       # YaÄŸÄ±ÅŸ miktarÄ±
                "temp_range",             # SÄ±caklÄ±k aralÄ±ÄŸÄ±
                "precipitation_range"     # YaÄŸÄ±ÅŸ aralÄ±ÄŸÄ± (varsa)
            ])
            if "population" in df.columns:
                mean_cols.append("population")
                
            def safe_mode(x):
                try: return x.mode().iloc[0]
                except: return np.nan

            agg_dict = {col: "mean" for col in mean_cols}
            agg_dict.update({col: safe_mode for col in mode_cols})
            agg_dict.update({"date": "min", "id": "count"})

            df["id"] = 1
            grouped = df.groupby(group_cols).agg(agg_dict).reset_index()
            grouped = grouped.rename(columns={"id": "crime_count"})
            grouped["Y_label"] = (grouped["crime_count"] >= 2).astype(int)

            geoids = df["GEOID"].unique()
            seasons = ["Winter", "Spring", "Summer", "Fall"]
            days = list(range(7))
            hours = list(range(24))
            expected_grid = pd.DataFrame(itertools.product(geoids, seasons, days, hours), columns=group_cols)

            df_final = expected_grid.merge(grouped, on=group_cols, how="left")
            df_final["crime_count"] = df_final["crime_count"].fillna(0).astype(int)
            df_final["Y_label"] = df_final["Y_label"].fillna(0).astype(int)

            df_final["is_weekend"] = df_final["day_of_week"].apply(lambda x: 1 if x >= 5 else 0)
            df_final["is_night"] = df_final["event_hour"].apply(lambda x: 1 if (x >= 20 or x < 4) else 0)
            df_final["is_school_hour"] = df_final.apply(lambda x: 1 if (x["day_of_week"] < 5 and 7 <= x["event_hour"] <= 16) else 0, axis=1)
            df_final["is_business_hour"] = df_final.apply(lambda x: 1 if (x["day_of_week"] < 6 and 9 <= x["event_hour"] < 18) else 0, axis=1)

            columns_with_nan = ["latitude", "longitude", "past_7d_crimes", "crime_count_past_24h", "crime_count_past_48h", "crime_trend_score", "prev_crime_1h", "prev_crime_2h", "prev_crime_3h", "is_holiday", "is_repeat_location", "year", "month", "date"]
            df_final = df_final.dropna(subset=columns_with_nan)

            existing_combinations = df_final[group_cols]
            missing = expected_grid.merge(existing_combinations.drop_duplicates(), on=group_cols, how="left", indicator=True)
            missing = missing[missing["_merge"] == "left_only"].drop(columns=["_merge"])
            missing["crime_count"] = 0
            missing["Y_label"] = 0

            df_full_52 = pd.concat([df_final, missing], ignore_index=True)

            df_final.to_csv("sf_crime_50.csv", index=False)
            df_full_52.to_csv("sf_crime_52.csv", index=False)
            df.to_csv("sf_crime.csv", index=False)
            st.success("âœ… TÃ¼m dosyalar baÅŸarÄ±yla kaydedildi: sf_crime.csv, sf_crime_50.csv, sf_crime_52.csv")

            # NaN raporu ve PDF
            nan_summary = df.isna().sum()
            nan_cols = nan_summary[nan_summary > 0]
            report_path = create_pdf_report("sf_crime.csv", original_row_count, nan_cols, len(df), removed_rows)
            with open(report_path, "rb") as f:
                st.download_button("ðŸ“„ PDF Raporu Ä°ndir", f, file_name=report_path, mime="application/pdf")
    
            # Ä°lk 5 satÄ±r, sÃ¼tunlar, NaN sayÄ±larÄ±
            st.write("### ðŸ“ˆ sf_crime.csv Ä°lk 5 SatÄ±r")
            st.dataframe(df.head())
            st.write("### ðŸ”¢ SÃ¼tunlar")
            st.write(df.columns.tolist())
            st.write("### ðŸ”” NaN SayÄ±larÄ±")
            st.write(nan_cols)
            st.write("ðŸ“¦ sf_crime.csv DosyasÄ±ndaki 911 SÃ¼tunlarÄ± ve Ä°lk SatÄ±rlar:")
            st.dataframe(df[cols_911 + ["GEOID", "datetime"]].head())

            st.subheader("ðŸ“Š ZenginleÅŸtirilmiÅŸ SuÃ§ Verisi (Ã–rnek)")
            st.write("ðŸ§© SÃ¼tunlar:")
            st.write(df.columns.tolist())
            
            st.write("ðŸ” Ä°lk 5 SatÄ±r:")
            st.dataframe(df.head())

# === 5 FONKSÄ°YON: Veri zenginleÅŸtirme ===
def enrich_with_poi(df):
    try:
        df_poi = pd.read_csv("sf_pois_cleaned_with_geoid.csv")
        with open("risky_pois_dynamic.json") as f:
            risk_dict = json.load(f)
        df_poi["risk_score"] = df_poi["poi_subcategory"].map(risk_dict).fillna(0)

        poi_features = df_poi.groupby("GEOID").agg(
            poi_total_count=("id", "count"),
            risky_poi_score=("risk_score", "mean")
        ).reset_index()

        df["GEOID"] = df["GEOID"].astype(str).str.zfill(11)
        poi_features["GEOID"] = poi_features["GEOID"].astype(str).str.zfill(11)
        df = df.merge(poi_features, on="GEOID", how="left")

        gdf_crime = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df["longitude"], df["latitude"]), crs="EPSG:4326").to_crs(3857)
        gdf_poi = gpd.GeoDataFrame(df_poi, geometry=gpd.points_from_xy(df_poi["lon"], df_poi["lat"]), crs="EPSG:4326").to_crs(3857)

        poi_coords = np.vstack([gdf_poi.geometry.x, gdf_poi.geometry.y]).T
        crime_coords = np.vstack([gdf_crime.geometry.x, gdf_crime.geometry.y]).T
        poi_tree = cKDTree(poi_coords)
        df["distance_to_poi"], _ = poi_tree.query(crime_coords, k=1)

        risky_poi = gdf_poi[gdf_poi["risk_score"] > 0]
        if not risky_poi.empty:
            risky_coords = np.vstack([risky_poi.geometry.x, risky_poi.geometry.y]).T
            risky_tree = cKDTree(risky_coords)
            df["distance_to_high_risk_poi"], _ = risky_tree.query(crime_coords, k=1)
        else:
            df["distance_to_high_risk_poi"] = np.nan

        risk_density = df_poi.groupby("GEOID")["risk_score"].mean().reset_index(name="poi_risk_density")
        risk_density["GEOID"] = risk_density["GEOID"].astype(str).str.zfill(11)
        df = df.merge(risk_density, on="GEOID", how="left")

        return df
    except Exception as e:
        st.error(f"POI zenginleÅŸtirme hatasÄ±: {e}")
        return df

def enrich_with_911(df):
    try:
        df_911 = pd.read_csv("sf_911_last_5_year.csv")
        df_911["date"] = pd.to_datetime(df_911["date"])
        df["date"] = pd.to_datetime(df["date"])
        df = df.merge(df_911, on=["GEOID", "date", "event_hour"], how="left")
        return df
    except Exception as e:
        st.error(f"911 verisi eklenemedi: {e}")
        return df

def enrich_with_311(df):
    try:
        df_311 = pd.read_csv("sf_311_last_5_years.csv")
        df_311["date"] = pd.to_datetime(df_311["date"])
        df["date"] = pd.to_datetime(df["date"])
        df = df.merge(df_311, on=["GEOID", "date", "event_hour"], how="left")
        return df
    except Exception as e:
        st.error(f"311 verisi eklenemedi: {e}")
        return df

def enrich_with_weather(df):
    try:
        df_weather = pd.read_csv("sf_weather_5years.csv")
        df_weather["DATE"] = pd.to_datetime(df_weather["DATE"])
        df["date"] = pd.to_datetime(df["date"])
        df = df.merge(df_weather, left_on="date", right_on="DATE", how="left")
        df.drop(columns=["DATE"], inplace=True)
        return df
    except Exception as e:
        st.error(f"Hava durumu verisi eklenemedi: {e}")
        return df

def enrich_with_police_and_gov(df):
    try:
        gdf_crime = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df["longitude"], df["latitude"]), crs="EPSG:4326").to_crs(3857)

        police_df = pd.read_csv("sf_police_stations.csv")
        gov_df = pd.read_csv("sf_government_buildings.csv")

        gdf_police = gpd.GeoDataFrame(police_df, geometry=gpd.points_from_xy(police_df["longitude"], police_df["latitude"]), crs="EPSG:4326").to_crs(3857)
        gdf_gov = gpd.GeoDataFrame(gov_df, geometry=gpd.points_from_xy(gov_df["longitude"], gov_df["latitude"]), crs="EPSG:4326").to_crs(3857)

        crime_coords = np.vstack([gdf_crime.geometry.x, gdf_crime.geometry.y]).T

        police_tree = cKDTree(np.vstack([gdf_police.geometry.x, gdf_police.geometry.y]).T)
        gov_tree = cKDTree(np.vstack([gdf_gov.geometry.x, gdf_gov.geometry.y]).T)

        df["distance_to_police"], _ = police_tree.query(crime_coords, k=1)
        df["distance_to_government_building"], _ = gov_tree.query(crime_coords, k=1)

        df["is_near_police"] = (df["distance_to_police"] < 200).astype(int)
        df["is_near_government"] = (df["distance_to_government_building"] < 200).astype(int)

        df["distance_to_police_range"] = pd.cut(df["distance_to_police"], bins=[0, 100, 200, 500, 1000, np.inf], labels=["0-100", "100-200", "200-500", "500-1000", ">1000"])
        df["distance_to_government_building_range"] = pd.cut(df["distance_to_government_building"], bins=[0, 100, 200, 500, 1000, np.inf], labels=["0-100", "100-200", "200-500", "500-1000", ">1000"])

        return df
    except Exception as e:
        st.error(f"Polis ve devlet binasÄ± hesaplama hatasÄ±: {e}")
        return df

# (app.py'deki ana kodlar burada devam eder)

# Ã–rnek test butonu:
if st.button("ðŸ§ª Veriyi GÃ¶ster (Test)"):
    df = pd.read_csv("sf_crime.csv")
    df["datetime"] = pd.to_datetime(df["date"].astype(str) + " " + df["time"].astype(str), errors="coerce")
    df["event_hour"] = df["datetime"].dt.hour
    df["date"] = df["datetime"].dt.date
    df = enrich_with_poi(df)
    df = enrich_with_911(df)
    df = enrich_with_311(df)
    df = enrich_with_weather(df)
    df = enrich_with_police_and_gov(df)
    df.to_csv("sf_crime.csv", index=False)

    st.success("âœ… sf_crime.csv baÅŸarÄ±yla zenginleÅŸtirildi.")
    st.write("ðŸ“Œ SÃ¼tunlar:", df.columns.tolist())
    st.dataframe(df.head())


